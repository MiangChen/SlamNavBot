import sounddevice as sd
import subprocess
import time

from scipy.io.wavfile import write
from faster_whisper import WhisperModel
from openai import OpenAI
import re, os
import pandas as pd
import openai

openai.api_key = "ä½ çš„OpenAI_API_KEY"

from unitree_sdk2py.core.channel import ChannelSubscriber, ChannelFactoryInitialize
from unitree_sdk2py.g1.audio.g1_audio_client import AudioClient
from unitree_sdk2py.g1.loco.g1_loco_client import LocoClient


class Speaker_unitree():
    def __init__(self, ip_g1: str = None):
        try:
            ChannelFactoryInitialize(0, ip_g1)
        except Exception as e:
            print("Exception init speaker:", e)

        self.audio_client = AudioClient()
        self.audio_client.SetTimeout(10.0)
        self.audio_client.Init()
        self.audio_client.SetVolume(85)

    def speak(self, speak_text):
        # green led
        self.audio_client.LedControl(0, 255, 0)
        self.audio_client.TtsMaker(speak_text, 0)
        time.sleep(len(speak_text) * 0.26)
        # blue led
        self.audio_client.LedControl(0, 0, 255)

    def get_volume(self):
        ret = self.audio_client.GetVolume()
        print("Volume: ", ret)
        return ret


def speak(text, voice='zh', speed=200):
    subprocess.run(["espeak", f"-v{voice}", f"-s{speed}", text])


speaker_unitree = Speaker_unitree(ip_g1="eno1")


# åˆå§‹åŒ– Whisper æ¨¡å‹ï¼ˆå¯ä»¥æ”¹æˆ "medium"ã€"large-v2"ï¼‰
model = WhisperModel("tiny", device="auto", compute_type="int8", cpu_threads=8)

# å€™é€‰ä½ç½®
candidate_locations = ['å†°ç®±', 'ç”µæ¢¯', 'å’–å•¡æœº', 'æ´—æ‰‹æ± ', 'åƒåœ¾æ¡¶', 'æœºå™¨äººå®éªŒå®¤']


def record_audio(filename="recording.wav", duration=3.5, fs=16000, ):
    print("ğŸ¤ å¼€å§‹å½•éŸ³ï¼Œè¯´è¯å§...")
    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')
    sd.wait()
    write(filename, fs, recording)
    print("âœ… å½•éŸ³å®Œæˆ")


def transcribe_audio(path):
    segments, info = model.transcribe(path, beam_size=5)
    text = "".join([seg.text for seg in segments])
    return text.strip()


def get_xyz_from_excel(location_name, excel_path='/home/zqh/yyx_G1/multiagent-isaacsim/llm/home.xlsx'):
    df = pd.read_excel(excel_path, header=None)  # ğŸ‘ˆ ä¸ä½¿ç”¨ç¬¬ä¸€è¡Œä½œä¸ºåˆ—å
    row = df[df.iloc[:, 0] == location_name]  # ç¬¬ä¸€åˆ—æ˜¯ä½ç½®å
    if not row.empty:
        x = row.iloc[0, 1]  # ç¬¬äºŒåˆ— X
        y = row.iloc[0, 2]  # ç¬¬ä¸‰åˆ— Y
        z = row.iloc[0, 3]  # ç¬¬å››åˆ— Z
        return x, y, z
    else:
        return None, None, None


def extract_location(text, candidates):
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªå¯¼èˆªåŠ©ç†ã€‚æˆ‘ä¼šç»™ä½ ä¸€å¥ä¸­æ–‡å‘½ä»¤ï¼Œè¯·ä½ ä»ä»¥ä¸‹å€™é€‰ä½ç½®ä¸­åˆ¤æ–­è¿™ä¸ªå‘½ä»¤æœ€å¯èƒ½æ˜¯åœ¨å“ªä¸ªä½ç½®æ‰§è¡Œçš„ï¼š
å€™é€‰ä½ç½®ï¼š{", ".join(candidates)}
å‘½ä»¤ï¼š{text}
è¯·ä½ åœ¨[]ä¸­åªè¿”å›ä¸€ä¸ªä½ç½®ï¼Œä¸è¦è§£é‡Šã€‚"""

    # client = OpenAI(api_key="sk-4b2b473f8d8149109dea837a488fa273", base_url="https://api.deepseek.com")
    client = OpenAI(api_key="sk-MQ8Zb5grxtM4hniFfyAWeblLcDxwNKG9tP64PGrfAaAePbr7", base_url="https://vip.dmxapi.com/v1")

    response = client.chat.completions.create(
        model="deepseek-chat",
        # model="gpt-4o",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå¯¼èˆªåŠ©ç†"},
            {"role": "user", "content": prompt},
        ],
        stream=False
    )
    return response.choices[0].message.content


def respond(text, location, ip_speaker: str = None):
    response = f"æˆ‘å°†å¸¦ä½ å»{location}"
    print("ğŸ¤– å›åº”ï¼š", response)
    if ip_speaker:
        speaker_unitree.speak(response)
    else:
        speak(response)


def extract_bracket_content(text):
    match = re.search(r'\[([^\[\]]+)\]', text)
    if match:
        return match.group(1)
    return None


def run_once(file_path: str = None, ip_speaker: str = None):
    if ip_speaker:
        speaker_unitree.speak("ä½ å¥½ï¼Œæˆ‘æ˜¯å¯¼èˆªæœºå™¨äººï¼Œä½ æœ‰ä»€ä¹ˆéœ€æ±‚")
    else:
        speak("ä½ å¥½ï¼Œæˆ‘æ˜¯å¯¼èˆªæœºå™¨äººï¼Œä½ æœ‰ä»€ä¹ˆéœ€æ±‚")
    if file_path is None:
        record_audio(duration=4)
        file_path = f"recording.wav"
    text = transcribe_audio(file_path)
    print("ğŸ“ ä½ è¯´çš„æ˜¯ï¼š", text)
    if not text:
        print("âŒ æ²¡å¬æ¸…")
        return
    location = extract_location(text, candidate_locations)
    location = extract_bracket_content(location)
    print(location)
    respond(text, location, ip_speaker=ip_speaker)
    x, y, z = get_xyz_from_excel(location)
    print(f'x: {x}')
    print(f'y: {y}')
    print(f'z: {z}')
    return x, y, z
