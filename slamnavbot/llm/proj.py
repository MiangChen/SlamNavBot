import sounddevice as sd
import numpy as np
from scipy.io.wavfile import write
from faster_whisper import WhisperModel
from openai import OpenAI
import re, os
import pandas as pd
import pyttsx3
import openai
from files.assets_scripts_linux import PATH_PROJECT  # è¿™é‡Œç”¨äº†é¡¹ç›®çš„è·¯å¾„ï¼Œ å¦‚æœæ‰¾ä¸åˆ°æŸä¸ªæ–‡ä»¶ï¼Œå¯èƒ½æ˜¯è¿™ä¸ªæ²¡è®¾ç½®
openai.api_key = "sk-MQ8Zb5grxtM4hniFfyAWeblLcDxwNKG9tP64PGrfAaAePbr7"
openai.api_base = "https://vip.dmxapi.com/L"

# åˆå§‹åŒ– TTS
engine = pyttsx3.init()
engine.setProperty('voice', 'com.apple.speech.synthesis.voice.ting-ting')  # mac ä¸Šçš„ä¸­æ–‡è¯­éŸ³

# åˆå§‹åŒ– Whisper æ¨¡å‹ï¼ˆå¯ä»¥æ”¹æˆ "medium"ã€"large-v2"ï¼‰
model = WhisperModel("small", device="cpu", compute_type="int8")

# å€™é€‰ä½ç½®
candidate_locations = ['å¨æˆ¿', 'å§å®¤', 'å®¢å…', 'å«ç”Ÿé—´', 'é˜³å°', "å’–å•¡æœº"]

# æ–‡ä»¶çš„æ ¹è·¯å¾„
def record_audio(filename="recording.wav", duration=5, fs=16000):
    print("ğŸ¤ å¼€å§‹å½•éŸ³ï¼Œè¯´è¯å§...")
    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')
    sd.wait()
    write(filename, fs, recording)
    print("âœ… å½•éŸ³å®Œæˆ")

def transcribe_audio(path):
    segments, info = model.transcribe(path, beam_size=5)
    text = "".join([seg.text for seg in segments])
    return text.strip()

def get_xyz_from_excel(location_name, excel_path=f'{PATH_PROJECT}/llm/home.xlsx'):
    df = pd.read_excel(excel_path, header=None)  # ğŸ‘ˆ ä¸ä½¿ç”¨ç¬¬ä¸€è¡Œä½œä¸ºåˆ—å
    row = df[df.iloc[:, 0] == location_name]     # ç¬¬ä¸€åˆ—æ˜¯ä½ç½®å
    if not row.empty:
        x = row.iloc[0, 1]  # ç¬¬äºŒåˆ— X
        y = row.iloc[0, 2]  # ç¬¬ä¸‰åˆ— Y
        z = row.iloc[0, 3]  # ç¬¬å››åˆ— Z
        return x, y, z
    else:
        return None, None, None

def extract_location(text, candidates):
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªå®¶å±…åŠ©ç†ã€‚æˆ‘ä¼šç»™ä½ ä¸€å¥ä¸­æ–‡å‘½ä»¤ï¼Œè¯·ä½ ä»ä»¥ä¸‹å€™é€‰ä½ç½®ä¸­åˆ¤æ–­è¿™ä¸ªå‘½ä»¤æœ€å¯èƒ½æ˜¯åœ¨å“ªä¸ªä½ç½®æ‰§è¡Œçš„ï¼š
å€™é€‰ä½ç½®ï¼š{", ".join(candidates)}
å‘½ä»¤ï¼š{text}
è¯·ä½ åœ¨[]ä¸­åªè¿”å›ä¸€ä¸ªä½ç½®ï¼Œä¸è¦è§£é‡Šã€‚"""
    # response = openai.ChatCompletion.create(
    #     model="gpt-3.5-turbo",
    #     messages=[{"role": "user", "content": prompt}]
    # )

    client = OpenAI(api_key="sk-4b2b473f8d8149109dea837a488fa273", base_url="https://api.deepseek.com")

    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå®¶å±…åŠ©ç†"},
            {"role": "user", "content": prompt},
        ],
        stream=False
    )
    # import pdb; pdb.set_trace()
    return response.choices[0].message.content

def respond(text, location):
    response = f"æˆ‘å°†å»{location}å®Œæˆï¼š{text}"
    print("ğŸ¤– å›åº”ï¼š", response)
    engine.say(response)
    engine.runAndWait()


def extract_bracket_content(text):
    match = re.search(r'\[([^\[\]]+)\]', text)
    if match:
        return match.group(1)
    return None

def run_once(file_path: str=None):
    if file_path is None:
        record_audio()
        file_path = f"recording.wav"
    text = transcribe_audio(f'{PATH_PROJECT}/llm/{file_path}')
    print("ğŸ“ ä½ è¯´çš„æ˜¯ï¼š", text)
    if not text:
        print("âŒ æ²¡å¬æ¸…")
        return
    location = extract_location(text, candidate_locations)
    location = extract_bracket_content(location)
    print(location)
    respond(text, location)
    x, y, z = get_xyz_from_excel(location)
    print(f'x: {x}')
    print(f'y: {y}')
    print(f'z: {z}')
    return x, y, z

# ğŸ‘‰ ä¸»ç¨‹åº
if __name__ == "__main__":
    while True:
        input("æŒ‰ä¸‹ Enter å¼€å§‹ä¸€æ¬¡å½•éŸ³ï¼ˆ5ç§’ï¼‰...")
        x, y, z = run_once()
